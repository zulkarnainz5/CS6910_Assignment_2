# -*- coding: utf-8 -*-
"""Assignment_DL_2B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yD4KyYtW1nCHV95XiiIwqHRH3E-KzLhJ
"""

import numpy as np
import torch
import torchvision
import torch.nn as nn
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
import torch.nn.init
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.functional import batch_norm
import matplotlib.pyplot as plt
from tqdm import tqdm

!pip install wandb
import wandb

!wandb login

entity_name="team_zulkar"

project_name="CS6910_Assignment_2A"

from google.colab import drive
drive.mount('/content/gdrive')

def unaug_data():
    data = '/content/gdrive/My Drive/inaturalist_12K'

    ## Unaugmented data


    data_transforms = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])


    #train dataset
    train_dataset = datasets.ImageFolder(root=f"{data}/train", transform=data_transforms)
    #test dataset
    test_dataset = datasets.ImageFolder(root=f"{data}/val", transform=data_transforms)

    initial_size = len(train_dataset)
    train_size = int(0.8 * initial_size)
    val_size = initial_size - train_size

    # Spliting the train dataset into train and validation data
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

    # Loading the dataset
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)

    return  train_loader, test_loader, val_loader

def aug_data():
    data = '/content/gdrive/My Drive/inaturalist_12K'

    ## augmented data


    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),
              transforms.RandomVerticalFlip(p=0.5),
              transforms.RandomRotation((120)),
              transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.5),
              transforms.Resize((224,224)),
              transforms.ToTensor(),
              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    test_transform = transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])


    #train dataset
    train_dataset = datasets.ImageFolder(root=f"{data}/train", transform=train_transform)
    #test dataset
    test_dataset = datasets.ImageFolder(root=f"{data}/val", transform=test_transform)

    initial_size = len(train_dataset)
    train_size = int(0.8 * initial_size)
    val_size = initial_size - train_size

    # Spliting the train dataset into train and validation data
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

    # Loading the dataset
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)
    return  train_loader, test_loader, val_loader

print(torch.cuda.is_available())

def Resnet_50():
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  model= models.resnet50(pretrained=True).to(device)

  for param in model.parameters():
    param.requires_grad = False

  num_inputs = model.fc.in_features
  model.fc = nn.Sequential(
               nn.Linear(num_inputs, 2048),
               nn.ReLU(inplace=True),
               nn.Dropout(0.3),
               nn.Linear(2048, 10))
  return model

def resnet_training(data_augmentation):

  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  model=Resnet_50().to(device)
  optimizer = optim.Adam(model.parameters(),lr=1e-3)
  criterion = nn.CrossEntropyLoss().to(device)

  if data_augmentation:
        train_loader, test_loader, val_loader = aug_data()
  else:
        train_loader, test_loader, val_loader = unaug_data()

  num_epochs=25
  gpu_train = torch.cuda.is_available()
  wandb.init()

  for epoch in tqdm(range(1, num_epochs+1)):
      train_loss = 0.0
      val_loss = 0.0
      val_accuracy = 0.0
      train_accuracy=0.0
      total=0.0
      correct=0.0


      model.train()
      for batch in train_loader:
              inputs, labels = batch
              if gpu_train:
                inputs, labels = inputs.cuda(), labels.cuda()
              optimizer.zero_grad()
              outputs = model(inputs)
              loss = criterion(outputs, labels)
              loss.backward()
              optimizer.step()
              train_loss += loss.item()*inputs.size(0)
              _, predicted = torch.max(outputs, 1)
              total += labels.size(0)
              correct += (predicted == labels).sum().item()

      train_accuracy = 100 * correct / total
      print(f"Train Accuracy: {train_accuracy:.4f}%")


      model.eval()
      for batch in val_loader:
            inputs, labels = batch
            if gpu_train:
              inputs, labels = inputs.cuda(), labels.cuda()
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()*inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

      val_accuracy = 100 * correct / total
      print(f"Validation Accuracy: {val_accuracy:.4f}%")


      train_loss = train_loss/len(train_loader.dataset)
      val_loss = val_loss/len(val_loader.dataset)
      val_accuracy = val_accuracy
      train_accuracy = train_accuracy

      wandb_log = True
      if(wandb_log==True):
        epoc=0
        log_dict = {"Train_loss": train_loss, "Validation_loss": val_loss, "Validation_Accuracy": val_accuracy,"Train_Accuracy": train_accuracy}
        epoc=epoc+1
        print('Epoch: {} \tTraining Loss: {:.4f} \tValidation Loss: {:.4f} \tValidation Accuracy: {:.4f} \Train Accuracy: {:.4f}'.format(epoc,
          train_loss, val_loss,val_accuracy, train_accuracy))

      wandb.log(log_dict)

  wandb.run.save()
  wandb.run.finish()
  return model

trained_model=resnet_training(False)

